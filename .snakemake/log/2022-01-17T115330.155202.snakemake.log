The flag 'directory' used in rule mark_duplicates is only valid for outputs, not inputs.
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cluster nodes: 300
Job counts:
	count	jobs
	2	check_contamination
	2	crosscheck_fingerprints
	1	main
	2	mark_duplicates
	2	sort_bam
	9

[Mon Jan 17 11:53:30 2022]
rule mark_duplicates:
    input: output/aligned_bam/1102781231/
    output: output/duplicates_marked/1102781231.aligned.unsorted.duplicates_marked.bam, output/duplicates_marked/1102781231.aligned.unsorted.duplicates_marked.duplicate_metrics
    jobid: 1
    wildcards: sample=1102781231


[Mon Jan 17 11:53:30 2022]
rule mark_duplicates:
    input: output/aligned_bam/1102777134/
    output: output/duplicates_marked/1102777134.aligned.unsorted.duplicates_marked.bam, output/duplicates_marked/1102777134.aligned.unsorted.duplicates_marked.duplicate_metrics
    jobid: 2
    wildcards: sample=1102777134

[Mon Jan 17 11:53:40 2022]
Error in rule mark_duplicates:
    jobid: 1
    output: output/duplicates_marked/1102781231.aligned.unsorted.duplicates_marked.bam, output/duplicates_marked/1102781231.aligned.unsorted.duplicates_marked.duplicate_metrics
    shell:
        
        module load java/1.8.0_211 python/3.7.3 picard/2.22.3

        #mkdir -p output/duplicates_marked/1102781231/
        bamlist=$(ls output/aligned_bam/1102781231/*.bam |  sed 's/output/INPUT=output/g' | sed 's/$/ \/') 
        java -Dsamjdk.compression_level=2 -Xms10g -jar $PICARD           MarkDuplicates           $bamlist           OUTPUT=output/duplicates_marked/1102781231.aligned.unsorted.duplicates_marked.bam           METRICS_FILE=output/duplicates_marked/1102781231.aligned.unsorted.duplicates_marked.duplicate_metrics           VALIDATION_STRINGENCY=SILENT           OPTICAL_DUPLICATE_PIXEL_DISTANCE=2500           ASSUME_SORT_ORDER="queryname"           CLEAR_DT="false"           ADD_PG_TAG_TO_READS=false
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Error executing rule mark_duplicates on cluster (jobid: 1, external: 403730, jobscript: /sc/arion/projects/MMAAAS/ngs_res/aaa_pilot_20201113/gatk20220113/.snakemake/tmp.wdnetrs9/snakejob.mark_duplicates.1.sh). For error details see the cluster log and the log files of the involved rule(s).
[Mon Jan 17 11:53:40 2022]
Error in rule mark_duplicates:
    jobid: 2
    output: output/duplicates_marked/1102777134.aligned.unsorted.duplicates_marked.bam, output/duplicates_marked/1102777134.aligned.unsorted.duplicates_marked.duplicate_metrics
    shell:
        
        module load java/1.8.0_211 python/3.7.3 picard/2.22.3

        #mkdir -p output/duplicates_marked/1102777134/
        bamlist=$(ls output/aligned_bam/1102777134/*.bam |  sed 's/output/INPUT=output/g' | sed 's/$/ \/') 
        java -Dsamjdk.compression_level=2 -Xms10g -jar $PICARD           MarkDuplicates           $bamlist           OUTPUT=output/duplicates_marked/1102777134.aligned.unsorted.duplicates_marked.bam           METRICS_FILE=output/duplicates_marked/1102777134.aligned.unsorted.duplicates_marked.duplicate_metrics           VALIDATION_STRINGENCY=SILENT           OPTICAL_DUPLICATE_PIXEL_DISTANCE=2500           ASSUME_SORT_ORDER="queryname"           CLEAR_DT="false"           ADD_PG_TAG_TO_READS=false
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Error executing rule mark_duplicates on cluster (jobid: 2, external: 403732, jobscript: /sc/arion/projects/MMAAAS/ngs_res/aaa_pilot_20201113/gatk20220113/.snakemake/tmp.wdnetrs9/snakejob.mark_duplicates.2.sh). For error details see the cluster log and the log files of the involved rule(s).
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /sc/arion/projects/MMAAAS/ngs_res/aaa_pilot_20201113/gatk20220113/.snakemake/log/2022-01-17T115330.155202.snakemake.log
