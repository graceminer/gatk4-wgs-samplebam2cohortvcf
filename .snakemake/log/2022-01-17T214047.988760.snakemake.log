The flag 'directory' used in rule mark_duplicates is only valid for outputs, not inputs.
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	2	check_contamination
	2	crosscheck_fingerprints
	1	main
	2	sort_bam
	7

[Mon Jan 17 21:40:48 2022]
rule sort_bam:
    input: output/duplicates_marked/1102781231.aligned.unsorted.duplicates_marked.bam
    output: output/sorted_bam/1102781231/1102781231.aligned.duplicates_marked.sorted.bam
    jobid: 3
    wildcards: sample=1102781231

Terminating processes on user request, this might take some time.
[Mon Jan 17 21:43:11 2022]
Error in rule sort_bam:
    jobid: 3
    output: output/sorted_bam/1102781231/1102781231.aligned.duplicates_marked.sorted.bam
    shell:
        
	module load R/3.5.3 java/1.8.0_211 python/3.7.3 picard/2.22.3

        java -Dsamjdk.compression_level=2 -Xms4000m -jar $PICARD           SortSam           INPUT=output/duplicates_marked/1102781231.aligned.unsorted.duplicates_marked.bam           OUTPUT=output/sorted_bam/1102781231/1102781231.aligned.duplicates_marked.sorted.bam           SORT_ORDER="coordinate"           CREATE_INDEX=true           CREATE_MD5_FILE=true           MAX_RECORDS_IN_RAM=300000
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job sort_bam since they might be corrupted:
output/sorted_bam/1102781231/1102781231.aligned.duplicates_marked.sorted.bam
Complete log: /sc/arion/projects/MMAAAS/ngs_res/aaa_pilot_20201113/gatk20220113/.snakemake/log/2022-01-17T214047.988760.snakemake.log
