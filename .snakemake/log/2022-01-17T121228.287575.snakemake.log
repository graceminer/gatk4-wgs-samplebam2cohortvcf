The flag 'directory' used in rule mark_duplicates is only valid for outputs, not inputs.
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	2	check_contamination
	2	crosscheck_fingerprints
	1	main
	2	mark_duplicates
	2	sort_bam
	9

[Mon Jan 17 12:12:28 2022]
rule mark_duplicates:
    input: output/aligned_bam/1102777134/
    output: output/duplicates_marked/1102777134.aligned.unsorted.duplicates_marked.bam, output/duplicates_marked/1102777134.aligned.unsorted.duplicates_marked.duplicate_metrics
    jobid: 2
    wildcards: sample=1102777134

Terminating processes on user request, this might take some time.
[Mon Jan 17 12:12:35 2022]
Error in rule mark_duplicates:
    jobid: 2
    output: output/duplicates_marked/1102777134.aligned.unsorted.duplicates_marked.bam, output/duplicates_marked/1102777134.aligned.unsorted.duplicates_marked.duplicate_metrics
    shell:
        
        module load java/1.8.0_211 python/3.7.3 picard/2.22.3

        #mkdir -p output/duplicates_marked/1102777134/
        bamlist=$(ls output/aligned_bam/1102777134/*.bam |  sed 's/output/INPUT=output/g')

        echo $bamlist
        
        java -Dsamjdk.compression_level=2 -Xms10g -jar $PICARD           MarkDuplicates           $bamlist           OUTPUT=output/duplicates_marked/1102777134.aligned.unsorted.duplicates_marked.bam           METRICS_FILE=output/duplicates_marked/1102777134.aligned.unsorted.duplicates_marked.duplicate_metrics           VALIDATION_STRINGENCY=SILENT           OPTICAL_DUPLICATE_PIXEL_DISTANCE=2500           ASSUME_SORT_ORDER="queryname"           CLEAR_DT="false"           ADD_PG_TAG_TO_READS=false
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Complete log: /sc/arion/projects/MMAAAS/ngs_res/aaa_pilot_20201113/gatk20220113/.snakemake/log/2022-01-17T121228.287575.snakemake.log
